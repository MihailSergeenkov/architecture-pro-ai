# Задание 1. Исследование моделей и инфраструктуры

## 1. Сравнение LLM-моделей

### Локальные модели (Hugging Face)

- Llama 4 Scout/Maverick — до 10M токенов контекста, оптимизированы для агентских приложений
- DeepSeek-V3 (671B параметров, 37B активных) — эффективное рассуждение
- Llama 3.3 70B — баланс качества и ресурсов, 128k контекст

| Критерий | Комментарий | Балл |
|----------|------------------|------|
| **Качество ответов** | 85-95% от GPT-4, отлично для специализированных задач после дообучения | 8/10 |
| **Скорость работы** | 10-100ms локально (без сети), с GPU ускорение 5-10x | 9/10 |
| **Стоимость владения и использования** | $30-36/GPU-день при загрузке 1M токенов/день | 9/10 |
| **Удобство и простота развёртывания** | Требует экспертизы в Kubernetes, управлении GPU, квантизации | 5/10 |

Преимущества:
- Полный контроль над данными
- Более точное обучение на внутренних документах
- Нет зависимости от внешних API и их лимитов
- Экономия при высокой утилизации

Недостатки:
- Высокая начальная инвестиция в железо
- Необходимость DevOps-экспертизы для мониторинга и обновлений

### Облачные модели (OpenAI, YandexGPT)

- OpenAI GPT-4.1 mini — \$0.80/1M входных токенов, \$3.20/1M выходных
- YandexGPT Pro 5.1 — \$0.00336/1k токенов (примерно \$3.36/1M)
- YandexGPT Lite — \$0.000834/1k токенов (примерно \$0.834/1M) для простых запросов

| Критерий | Комментарий | Балл |
|----------|-----------------|------|
| **Качество ответов** | Отличное, лучшая связность диалога | 10/10 |
| **Скорость работы** | 100-500ms с учётом сети, батчинг до 2048 запросов | 7/10 |
| **Стоимость владения и использования** | \$0.12-0.30/1M токенов (API), выгодно при низкой нагрузке (<25%) | 7/10 |
| **Удобство и простота развёртывания** | Простая REST API интеграция, без управления инфраструктурой | 10/10 |

Преимущества:
- Моментальный старт без капитальных затрат
- Постоянные обновления моделей без миграции
- Предсказуемые операционные расходы

Недостатки:
- Данные уходят во внешние сервисы — риск для конфиденциальной документации
- Дороже локальных при объёме >10M токенов/день
- Вендор-лок и зависимость от кол-ва запросов

## 2. Сравнение моделей эмбеддингов

### Локальные (Sentence-Transformers)

- all-mpnet-base-v2 — базовый вариант, 89% точности в тестах
- multilingual-e5-large — для мультиязычной документации

| Критерий | Комментарий | Оценка |
|----------|----------------------|--------|
| **Скорость создания индекса** | 10-100ms на документ локально, масштабируется с GPU | 9/10 |
| **Качество поиска** | MTEB score примерно 62-64, достаточно для корпоративного поиска | 8/10 |
| **Стоимость владения и использования** | Нулевая после инвестиции в железо, входит в общую инфраструктуру | 10/10 |

Преимущества:
- Возможность переиндексации 18k файлов без API-затрат
- Приватность при обработке конфиденциальных документов
- Сотни предобученных моделей для разных задач

### Облачные (OpenAI Embeddings)

- text-embedding-3-large — MTEB score 64.6, лучший по точности
- text-embedding-3-small — MTEB 62.3, дешевле

| Критерий | Комментарий | Оценка |
|----------|-------------------|--------|
| **Скорость создания индекса** | 100-500ms с учётом сети, батчинг 2048 документов | 7/10 |
| **Качество поиска** | Топовое (MTEB 64.6), лучше извлекает семантику | 10/10 |
| **Стоимость владения и использования** | примерно \$0.13/1M токенов для small, первичная индексация 18k файлов примерно \$50-100 | 6/10 |

Недостатки:
- Реиндексация при обновлениях требует повторной оплаты

## 3. Сравнение векторных баз данных

### FAISS

| Параметр | Характеристика |
|----------|----------------|
| **Скорость поиска** | Максимальная (C++ реализация), обрабатывает миллиарды векторов | 
| **Индексация** | Быстрая с GPU-ускорением (5-10x), квантизация уменьшает память на 50-75% |
| **Сложность внедрения и поддержки** | Требует ручной реализации персистентности, ограниченная поддержка метаданных |
| **Удобство в работе** | Максимальная производительность при миллионах документов, интеграция с NumPy/PyTorch |
| **Стоимость владения** | Только инфраструктура (RAM), высокая оптимизация памяти |

### ChromaDB

| Параметр | Характеристика |
|----------|----------------|
| **Скорость поиска** | Достаточная для большинства сценариев (миллионы векторов), Python-реализация |
| **Индексация** | Встроенная персистентность, автоматическое управление |
| **Сложность внедрения и поддержки** | Простая интеграция, подходит для начинающих, хорошая поддержка метаданных |
| **Удобство в работе** | Быстрая разработка RAG, прототипирование, веб-приложения с комплексной фильтрацией |
| **Стоимость владения** | Дополнительные рассходы из-за реализации на Python, но проще в поддержке |

## 4. Конфигурация серверов

### Вариант 1: Полностью облачный (Быстрый старт)
Стек:
- LLM: OpenAI GPT-4.1 mini или YandexGPT Pro 5.1
- Embeddings: OpenAI text-embedding-3-small
- Vector DB: ChromaDB на AWS EKS
- Инфраструктура: текущий Kubernetes кластер

Бюджет:
- Начальный: \$0 (используется существующая инфра)
- Операционный: примерно \$200-400/месяц при 5-10M токенов

Плюсы: запуск за 1-2 недели, минимальные риски, легко масштабируется
Минусы: данные в облаке (риск по КБ), долгосрочно дороже при росте

### Вариант 2: Гибридный (С утранением рисков по КБ)
Стек:
- LLM: Llama 3.3 70B локально (конфиденциальные запросы) + YandexGPT Lite (публичная документация)
- Embeddings: multilingual-e5-large локально
- Vector DB: ChromaDB с локальным хранением
- Инфраструктура: выделенный GPU-сервер + AWS EKS

Бюджет:
- Начальный: \$15-20k (железо) + 4-6 недель разработки
- Операционный: примерно \$100-150/месяц (YandexGPT для некритичных запросов)

Плюсы:
- Баланс безопасности и удобства
- Конфиденциальные данные не покидают инфраструктуру
- Точное обучение на специфичной документации
Минусы: требуется DevOps для GPU-кластера, квантизации моделей

### Вариант 3: Полностью локальный (Полный контроль)
Стек:
- LLM: Llama 4 Maverick (70B) с точным обучением
- Embeddings: multilingual-e5-large
- Vector DB: FAISS (производительность) + ChromaDB (метаданные)
- Инфраструктура: vps или bare-metal

Бюджет:
- Начальный: \$25-35k (hardware) + 8-10 недель разработки
- Операционный: примерно \$200-300/месяц (электричество, поддержка)

Плюсы:
- Полная автономность и контроль
- Нет постоянных API-затрат
Минусы: высокие начальные инвестиции, долгая окупаемость (12-18 месяцев)

### Выбранный вариант для реального использования

Вариант 2 (Гибридный) оптимален для рассматриваемого случая по следующим причинам:

1. Соответствие требования по КБ: конфиденциальные данные обрабатываются локально
2. Экономика: окупаемость через 8-12 месяцев при текущем объёме (18k файлов + 400/месяц прироста)
3. Технический стек: команда уже работает с Kubernetes, Docker, можно переиспользовать AWS EKS
4. Масштабирование: архитектура позволит в будущем предлагать RAG-решение клиентам
5. Онбординг новых сотрудников: сокращение 4 часов поиска/неделю на 50+ инженеров = примерно \$150k/год экономии ФОТ

### Выбранный вариант для учебного использования

Вариант 2 с исключением облачных решений:

1. Embeddings: paraphrase-multilingual-MiniLM-L12-v2 локально, тк нет GPU на используемой рабочей станции
2. Vector DB: ChromaDB с локальным хранением

# Задание 2. Подготовка базы знаний

Исходные файлы находятся в директории [source_base](./source_base/)
Скрипт [process_characters.py](./process_characters.py)
Итоговая база находятся в директории [knowledge_base](./knowledge_base/)
Словарь замен [json](./terms_map.json)

# Задание 3. Создание векторного индекса базы знаний

## 1. Выбор эмбеддинг-модели 

- Название - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
- Ссылка - https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
- Размер эмбеддингов - 384-мерные эмбеддинги

## 2. База знаний 

- Взята из директории [knowledge_base](./knowledge_base/) - 30 файлов с подменами


## 3. Кол-во чанков в индексе

- 1775 чанков

## 4. Время генерации

- примерно 20 секунд.

## 5. Использование

- Для создания индекса и выполнить запрос - python build_index.py --build
- Просто выполнить запрос - python build_index.py --test

# Задание 4. Реализация RAG-бота с техниками промптинга

- [RAG модуль](./rag.py)
- [Telegram Bot](./telegram_bot.py) Запуск TELEGRAM_BOT_TOKEN=test python telegram_bot.py
- ![success1](./task4/success1.png)
- ![success2](./task4/success2.png)
- ![success3](./task4/success3.png)
- ![failed1](./task4/failed1.png)
- ![failed2](./task4/failed2.png)

# Задание 5. Запуск и демонстрация работы бота

![success1](./task5/success1.png)
![success2](./task5/success2.png)
![success3](./task5/success3.png)
![success4](./task5/success4.png)
![success5](./task5/success5.png)
![failed1](./task5/failed1.png)
![failed2](./task5/failed2.png)
![failed3](./task5/failed3.png)
![failed4](./task5/failed4.png)
![failed5](./task5/failed5.png)

1. Расширил системный промт SECURITY_SYSTEM_PROMPT, добавил инструкции для соблюдения безопасности.
2. Добавил дополнительную фильтрацию вывода. Если ранее бот мог что-то сказать про root (во всяком случае говорил, что информация найдена, но не выводил), то теперь из-за удаления информации, говорит "Я не знаю".

# Задание 6. Автоматическое ежедневное обновление базы знаний

## 1. Скрипт
Скрипт [update_index.py](./update_index.py) выполняет:
- Отслеживание файлов: метаданные сохраняются в indexed_files.json (путь + mtime)
- Обнаружение изменений: сравнивает текущие файлы с проиндексированными
- Разбиение на чанки: используется RecursiveCharacterTextSplitter
- Обновление индекса: добавляет новые/изменённые документы, удаляет старые чанки при изменении
- Логирование: логируетяс полный процесс + итоговый результат

## 2. Периодический запуск
Через обычный cron c добавлением записи 
```
0 0 * * * python update_index.py # раз в сутки в полночь
```

## 3. Диаграмма

[Диаграмма](./architecture.puml)

## 4. Лог

```
index updated at 2026-02-15, 1 files added, 1 new chunks, index size: 1780, 0 errors
```

# Задание 7. Аналитика покрытия и качества базы знаний

- Лог: [request_logs.csv](./request_logs.csv)
- Набор вопросов и ответов: [golden_questions.json](./golden_questions.json)
- Скрипт тестирования модели: [test_model.py](./test_model.py)
- Диаграмма последовательностей: [sequence_diagram.puml](./sequence_diagram.puml)

Результат выполнения скрипта показал практически полное покрытие (соответствие ожидаемым ответам) - 95.4%.
