# Задание 1. Исследование моделей и инфраструктуры

## 1. Сравнение LLM-моделей

### Локальные модели (Hugging Face)

- Llama 4 Scout/Maverick — до 10M токенов контекста, оптимизированы для агентских приложений
- DeepSeek-V3 (671B параметров, 37B активных) — эффективное рассуждение
- Llama 3.3 70B — баланс качества и ресурсов, 128k контекст

| Критерий | Комментарий | Балл |
|----------|------------------|------|
| **Качество ответов** | 85-95% от GPT-4, отлично для специализированных задач после дообучения | 8/10 |
| **Скорость работы** | 10-100ms локально (без сети), с GPU ускорение 5-10x | 9/10 |
| **Стоимость владения и использования** | $30-36/GPU-день при загрузке 1M токенов/день | 9/10 |
| **Удобство и простота развёртывания** | Требует экспертизы в Kubernetes, управлении GPU, квантизации | 5/10 |

Преимущества:
- Полный контроль над данными
- Более точное обучение на внутренних документах
- Нет зависимости от внешних API и их лимитов
- Экономия при высокой утилизации

Недостатки:
- Высокая начальная инвестиция в железо
- Необходимость DevOps-экспертизы для мониторинга и обновлений

### Облачные модели (OpenAI, YandexGPT)

- OpenAI GPT-4.1 mini — \$0.80/1M входных токенов, \$3.20/1M выходных
- YandexGPT Pro 5.1 — \$0.00336/1k токенов (примерно \$3.36/1M)
- YandexGPT Lite — \$0.000834/1k токенов (примерно \$0.834/1M) для простых запросов

| Критерий | Комментарий | Балл |
|----------|-----------------|------|
| **Качество ответов** | Отличное, лучшая связность диалога | 10/10 |
| **Скорость работы** | 100-500ms с учётом сети, батчинг до 2048 запросов | 7/10 |
| **Стоимость владения и использования** | \$0.12-0.30/1M токенов (API), выгодно при низкой нагрузке (<25%) | 7/10 |
| **Удобство и простота развёртывания** | Простая REST API интеграция, без управления инфраструктурой | 10/10 |

Преимущества:
- Моментальный старт без капитальных затрат
- Постоянные обновления моделей без миграции
- Предсказуемые операционные расходы

Недостатки:
- Данные уходят во внешние сервисы — риск для конфиденциальной документации
- Дороже локальных при объёме >10M токенов/день
- Вендор-лок и зависимость от кол-ва запросов

## 2. Сравнение моделей эмбеддингов

### Локальные (Sentence-Transformers)

- all-mpnet-base-v2 — базовый вариант, 89% точности в тестах
- multilingual-e5-large — для мультиязычной документации

| Критерий | Комментарий | Оценка |
|----------|----------------------|--------|
| **Скорость создания индекса** | 10-100ms на документ локально, масштабируется с GPU | 9/10 |
| **Качество поиска** | MTEB score примерно 62-64, достаточно для корпоративного поиска | 8/10 |
| **Стоимость владения и использования** | Нулевая после инвестиции в железо, входит в общую инфраструктуру | 10/10 |

Преимущества:
- Возможность переиндексации 18k файлов без API-затрат
- Приватность при обработке конфиденциальных документов
- Сотни предобученных моделей для разных задач

### Облачные (OpenAI Embeddings)

- text-embedding-3-large — MTEB score 64.6, лучший по точности
- text-embedding-3-small — MTEB 62.3, дешевле

| Критерий | Комментарий | Оценка |
|----------|-------------------|--------|
| **Скорость создания индекса** | 100-500ms с учётом сети, батчинг 2048 документов | 7/10 |
| **Качество поиска** | Топовое (MTEB 64.6), лучше извлекает семантику | 10/10 |
| **Стоимость владения и использования** | примерно \$0.13/1M токенов для small, первичная индексация 18k файлов примерно \$50-100 | 6/10 |

Недостатки:
- Реиндексация при обновлениях требует повторной оплаты

## 3. Сравнение векторных баз данных

### FAISS

| Параметр | Характеристика |
|----------|----------------|
| **Скорость поиска** | Максимальная (C++ реализация), обрабатывает миллиарды векторов | 
| **Индексация** | Быстрая с GPU-ускорением (5-10x), квантизация уменьшает память на 50-75% |
| **Сложность внедрения и поддержки** | Требует ручной реализации персистентности, ограниченная поддержка метаданных |
| **Удобство в работе** | Максимальная производительность при миллионах документов, интеграция с NumPy/PyTorch |
| **Стоимость владения** | Только инфраструктура (RAM), высокая оптимизация памяти |

### ChromaDB

| Параметр | Характеристика |
|----------|----------------|
| **Скорость поиска** | Достаточная для большинства сценариев (миллионы векторов), Python-реализация |
| **Индексация** | Встроенная персистентность, автоматическое управление |
| **Сложность внедрения и поддержки** | Простая интеграция, подходит для начинающих, хорошая поддержка метаданных |
| **Удобство в работе** | Быстрая разработка RAG, прототипирование, веб-приложения с комплексной фильтрацией |
| **Стоимость владения** | Дополнительные рассходы из-за реализации на Python, но проще в поддержке |

## 4. Конфигурация серверов

### Вариант 1: Полностью облачный (Быстрый старт)
Стек:
- LLM: OpenAI GPT-4.1 mini или YandexGPT Pro 5.1
- Embeddings: OpenAI text-embedding-3-small
- Vector DB: ChromaDB на AWS EKS
- Инфраструктура: текущий Kubernetes кластер

Бюджет:
- Начальный: \$0 (используется существующая инфра)
- Операционный: примерно \$200-400/месяц при 5-10M токенов

Плюсы: запуск за 1-2 недели, минимальные риски, легко масштабируется
Минусы: данные в облаке (риск по КБ), долгосрочно дороже при росте

### Вариант 2: Гибридный (С утранением рисков по КБ)
Стек:
- LLM: Llama 3.3 70B локально (конфиденциальные запросы) + YandexGPT Lite (публичная документация)
- Embeddings: multilingual-e5-large локально
- Vector DB: ChromaDB с локальным хранением
- Инфраструктура: выделенный GPU-сервер + AWS EKS

Бюджет:
- Начальный: \$15-20k (железо) + 4-6 недель разработки
- Операционный: примерно \$100-150/месяц (YandexGPT для некритичных запросов)

Плюсы:
- Баланс безопасности и удобства
- Конфиденциальные данные не покидают инфраструктуру
- Точное обучение на специфичной документации
Минусы: требуется DevOps для GPU-кластера, квантизации моделей

### Вариант 3: Полностью локальный (Полный контроль)
Стек:
- LLM: Llama 4 Maverick (70B) с точным обучением
- Embeddings: multilingual-e5-large
- Vector DB: FAISS (производительность) + ChromaDB (метаданные)
- Инфраструктура: vps или bare-metal

Бюджет:
- Начальный: \$25-35k (hardware) + 8-10 недель разработки
- Операционный: примерно \$200-300/месяц (электричество, поддержка)

Плюсы:
- Полная автономность и контроль
- Нет постоянных API-затрат
Минусы: высокие начальные инвестиции, долгая окупаемость (12-18 месяцев)

### Выбранный вариант для реального использования

Вариант 2 (Гибридный) оптимален для рассматриваемого случая по следующим причинам:

1. Соответствие требования по КБ: конфиденциальные данные обрабатываются локально
2. Экономика: окупаемость через 8-12 месяцев при текущем объёме (18k файлов + 400/месяц прироста)
3. Технический стек: команда уже работает с Kubernetes, Docker, можно переиспользовать AWS EKS
4. Масштабирование: архитектура позволит в будущем предлагать RAG-решение клиентам
5. Онбординг новых сотрудников: сокращение 4 часов поиска/неделю на 50+ инженеров = примерно \$150k/год экономии ФОТ

### Выбранный вариант для учебного использования

Вариант 2 с исключением облачных решений:

1. Embeddings: multilingual-e5-large локально
2. Vector DB: ChromaDB с локальным хранением

